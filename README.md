# ğŸ™ï¸ Crowd Density Classification using CNNs ğŸ¤–

Welcome to the **Crowd Density Classification** project! ğŸš€ This repository provides deep learning solutions using **CNN-based models** to classify crowd density levels in images from the **JHU-CROWD++** and **ShanghaiTech** datasets. ğŸ“·

---

## ğŸ“‚ Project Overview

We present solutions for crowd density estimation using two widely known datasets:

- **JHU-CROWD++ Dataset ğŸ›ï¸**
  - Classifies images into three density levels based on head counts.
  - Utilizes convolutional neural networks with **dropout** and **early stopping** to enhance performance.

- **ShanghaiTech Dataset ğŸ¢**
  - Provides detailed head count data via ground truth annotations (.mat files).
  - Includes **preprocessing**, **CNN training**, and **visualization**.

---

## ğŸš€ Features

âœ¨ **End-to-End Pipeline:** Image preprocessing, training, evaluation, and prediction.

âœ¨ **CNN Architecture:** Multi-layered convolutional networks optimized for crowd analysis.

âœ¨ **Early Stopping:** Prevents overfitting by monitoring validation performance.

âœ¨ **Learning Rate Reduction:** Automatically adjusts learning rates to improve convergence.

âœ¨ **Visualization:** Accuracy and loss graphs to track progress.

âœ¨ **Customizable:** Modify dataset paths and parameters with ease.

---

## ğŸ› ï¸ Prerequisites

Ensure the following dependencies are installed before running the project:

**Python 3.x ğŸ**

### Required Libraries:

```bash
pip install tensorflow numpy pandas matplotlib pillow scipy
```

---

## ğŸ“ Project Structure

```
crowd-density-classification/
â”‚-- JHU_CROWD++/           # Files related to JHU-CROWD++ dataset
â”‚   â”œâ”€â”€ jhucrowdpp_cnn.py   # Model training and evaluation script
â”‚   â”œâ”€â”€ Readme1.md          # JHU dataset files (images and labels)
â”‚-- ShanghaiTech/          # Files related to ShanghaiTech dataset
â”‚   â”œâ”€â”€ shanghaitech_cnn.py # Model training and evaluation script
â”‚   â”œâ”€â”€ Readme2.md          # ShanghaiTech dataset files (images and labels)
â”‚-- README.md              # Project documentation
```

---

## âš™ï¸ How to Run

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/yourusername/crowd-density-classification.git
cd crowd-density-classification
```

### 2ï¸âƒ£ Modify Dataset Paths

Update dataset paths in respective scripts:

```python
# JHU-CROWD++ dataset
dataset_path = r"C:\JHU_CROWD++_DATASET\train\images"
labels_file_path = r"C:\JHU_CROWD++_DATASET\train\image_labels.txt"

# ShanghaiTech dataset
dataset_path = r"C:\shanghaitech\part_A\train_data\images"
ground_truth_path = r"C:\shanghaitech\part_A\train_data\ground-truth"
```

### 3ï¸âƒ£ Train the Models ğŸ‹ï¸â€â™‚ï¸

#### For JHU-CROWD++ Dataset:

```bash
python JHU_CROWD++/jhucrowdpp_cnn.py
```

#### For ShanghaiTech Dataset:

```bash
python ShanghaiTech/shanghaitech_cnn.py
```

### 4ï¸âƒ£ Evaluate the Model ğŸ§ª

Upon training completion, accuracy and loss metrics will be displayed.

### 5ï¸âƒ£ Make Predictions ğŸ”®

```python
from cnn_model import predict_image

predicted_label = predict_image("cnn_model.h5", "test_image.jpeg")
print(f"Predicted Crowd Density Class: {predicted_label}")
```

---

## ğŸ—ï¸ Model Architectures

### JHU-CROWD++ CNN Model ğŸ›ï¸

- **Input:** 224x224 RGB Images
- **Layers:**
  - Conv2D (32 filters, 3x3 kernel) â¡ï¸ MaxPooling (2x2) â¡ï¸ Dropout (0.1)
  - Conv2D (64 filters, 3x3 kernel) â¡ï¸ MaxPooling (2x2) â¡ï¸ Dropout (0.2)
  - Conv2D (128 filters, 3x3 kernel) â¡ï¸ MaxPooling (2x2) â¡ï¸ Dropout (0.3)
  - Fully Connected Layer â¡ï¸ Dropout (0.7) â¡ï¸ Output (Softmax for 3 classes)

### ShanghaiTech CNN Model ğŸ¢

- **Input:** 224x224 RGB Images
- **Layers:**
  - Conv2D (32 filters, 3x3 kernel) â¡ï¸ MaxPooling (2x2) â¡ï¸ Dropout (0.3)
  - Conv2D (64 filters, 3x3 kernel) â¡ï¸ MaxPooling (2x2) â¡ï¸ Dropout (0.4)
  - Fully Connected Layer â¡ï¸ Dropout (0.6) â¡ï¸ Output (Softmax for 3 classes)

---

## ğŸ“Š Results

- Both models achieve **satisfactory performance** on their respective test sets.
- Training stops early if no improvements are observed.

### ğŸ“ˆ Visualization

Training and validation accuracy/loss graphs are plotted for better analysis.

---

## âš¡ Performance Improvements

The following techniques are employed to enhance performance:

- **Dropout:** Prevents overfitting during training.
- **Early Stopping:** Stops training if validation loss stops improving.
- **Learning Rate Reduction:** Adjusts learning rates based on validation performance.
- **Batch Normalization:** Helps with faster convergence and improved accuracy.

---

## ğŸ“ Acknowledgments

Special thanks to the creators of the **JHU-CROWD++** and **ShanghaiTech** datasets. ğŸ™

---

## ğŸ“§ Contact

For any questions, feel free to reach out:

- ğŸ“© Email: [nekunj44@gmail.com](mailto:nekunj44@gmail.com)
- ğŸŒ GitHub: [nekunj44](https://github.com/nekunj44)

---

ğŸŒŸ **Don't forget to star this repo if you found it useful!** â­